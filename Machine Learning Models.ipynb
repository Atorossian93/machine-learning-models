{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DECISION TREE CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decisionTree_clf(feature_columns,target_column,data_frame,os_thres,*args,**kwargs):\n",
    "    '''\n",
    "    The decisionTree_clf function is for executing a classifier model based on a certain amount of features of a dataset.\n",
    "    This function has the purpose of returning a complete performance analysis of a certain model. The function will\n",
    "    generate accuracy score, Jaccard score, recall score and precision score, ROC-AUC curves, classification report and\n",
    "    confusion matrix of the executed model.\n",
    "    The function parameters you should pass are:\n",
    "    - feature_columns: list of str, default=None\n",
    "        It is the list of features that the model will evaluate to predict the target or class\n",
    "    - target_column: str, default=None\n",
    "        It is the target variable or class to predict with the model\n",
    "    - data_frame: str, default=None\n",
    "        It is the name of the dataframe that the feature_columns and the target_column belong to.\n",
    "    - os.thres: float, default=None\n",
    "        Represents the threshold up to which the oversampling won't be needed given the ratio between the minority \n",
    "        and the majority class. The threshold is represented with a float between 0 and 1. \n",
    "    '''\n",
    "    \n",
    "    # Importing libraries\n",
    "    import numpy as np                                   # for handling mathematical operations \n",
    "    import pandas as pd                                  # for building and handling dataframe operations\n",
    "    import matplotlib.pyplot as plt                      # for generating plots\n",
    "    import seaborn as sns                                # for generating plots\n",
    "    from sklearn import tree                             # for building the models and generating the tree graph\n",
    "    from imblearn.over_sampling import SMOTE             # for generating an oversampling dataset\n",
    "    from sklearn.model_selection import train_test_split # for slicing the dataframe in testing and training\n",
    "    from sklearn import metrics                          # for returning the performance metrics of the model\n",
    "    from sklearn import preprocessing                    # for \n",
    "    \n",
    "    def title(symbol,string,width):\n",
    "        len_str = len(string)\n",
    "        len_sym = width-len_str\n",
    "        len_sym = int(len_sym/2)\n",
    "        title = symbol*len_sym+string+symbol*len_sym\n",
    "        print(title)\n",
    "    \n",
    "    # Defining target and features variables\n",
    "    X = data_frame[feature_columns]   # Features\n",
    "    y = data_frame[target_column]     # Target\n",
    "    \n",
    "    # Generating Oversampling\n",
    "    # Generating Oversampling\n",
    "    min_y=min(list(y.value_counts())) # Minoritary class\n",
    "    max_y=max(list(y.value_counts())) # Majoritary class\n",
    "    min_ratio=min_y/max_y             # ratio of the minoritary class by the majoritary class\n",
    "    title('-','Target variable sampling ratio',124)\n",
    "    if os_thres>min_ratio:\n",
    "        print('Minority Class: '+str(min_y)+\n",
    "              '\\nMajority Class: '+str(max_y)+\n",
    "              '\\nMinority Proportion: '+str(round(min_ratio,2)))\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20) # 70% training and 30% test\n",
    "        os = SMOTE(sampling_strategy='auto',random_state=0,k_neighbors=5)\n",
    "        X_train, y_train = os.fit_resample(X_train, y_train)\n",
    "        title('-','Oversampling generated with SMOTE technique',124)\n",
    "        fig,ax=plt.subplots(1,2,figsize=(10,5))\n",
    "        ax[0].bar(y.value_counts().index,y.value_counts().values,color=['darkorange','blue'],alpha=0.8)\n",
    "        ax[0].set_ylabel('Count')\n",
    "        ax[0].set_xlabel('Class')\n",
    "        ax[0].set_title('Countplot of Each Class before oversampling\\nwithin the Target Variable')\n",
    "        ax[1].bar(y_train.value_counts().index,y_train.value_counts().values,color=['darkorange','blue'],alpha=0.8)\n",
    "        ax[1].set_ylabel('Count')\n",
    "        ax[1].set_xlabel('Class')\n",
    "        ax[1].set_title('Countplot of Each Class after oversampling\\nwithin the Target Variable')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('Minority Class: '+str(min_y)+\n",
    "              '\\nMajority Class: '+str(max_y)+\n",
    "              '\\nMinority Proportion: '+str(round(min_ratio,2)))\n",
    "        title('-','No Oversampling generated',124)\n",
    "        sns.countplot(y)\n",
    "        plt.ylabel('Count')\n",
    "        plt.xlabel('Class')\n",
    "        plt.title('Countplot of Each Class \\nwithin the Target Variable')\n",
    "        plt.show()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30) # 70% training and 30% test\n",
    "    \n",
    "    # Creating Decision Tree Classifier\n",
    "    model='Decision Tree Classifier'\n",
    "    clf = tree.DecisionTreeClassifier(random_state=0,*args,**kwargs)\n",
    "    ## Training Decision Tree Classifer\n",
    "    clf = clf.fit(X_train,y_train)\n",
    "    ## Predicting the response for test dataset\n",
    "    y_pred = clf.predict(X_test)\n",
    "       \n",
    "    # Generating metrics for the executed model\n",
    "    title('-','Model Performance Metrics',124)\n",
    "    print('Accuracy: %0.2f'%round(metrics.accuracy_score(y_test, y_pred),2))\n",
    "    print('Jaccard: %0.2f'%round(metrics.jaccard_score(y_test, y_pred,pos_label=1),2))\n",
    "    print('Precision: %0.2f'%round(metrics.precision_score(y_test, y_pred,pos_label=1),2))\n",
    "    print('Recall: %0.2f'%round(metrics.recall_score(y_test, y_pred,pos_label=1),2))\n",
    "    print('\\n')\n",
    "\n",
    "    # Generating classification report\n",
    "    title('-','Classification Report',124)\n",
    "    print(metrics.classification_report(y_test, y_pred))\n",
    "    print('\\n')\n",
    "     \n",
    "    # Generating Confusion Matrix\n",
    "    title('-','Confusion Matrix',124)\n",
    "    cm_t = metrics.confusion_matrix(y_test, y_pred)\n",
    "    with sns.axes_style(\"white\"):\n",
    "        fig, ax = plt.subplots(figsize=(4,4))\n",
    "        ax = sns.heatmap(cm_t, \n",
    "                        square=True,\n",
    "                        annot=True,\n",
    "                        fmt=\"d\",\n",
    "                        cbar=False)\n",
    "        ax.xaxis.set_ticklabels(['Negative', 'Positive'])\n",
    "        ax.yaxis.set_ticklabels(['Negative', 'Positive']);\n",
    "        ax.set_xlabel('Predicted')\n",
    "        ax.set_ylabel('True')\n",
    "    plt.title('Confusion Matrix of the {m}'.format(m=model))\n",
    "    plt.show()\n",
    "    print('\\n')\n",
    "    \n",
    "    # Generating and plotting ROC-AUC Curve\n",
    "    title('-','ROC-AUC Curve',124)\n",
    "    logit_roc_auc = metrics.roc_auc_score(y_test, y_pred)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred,pos_label=1)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label='{m} (Area = %0.2f)'.format(m=model)%logit_roc_auc)\n",
    "    plt.plot([0,1],[0,1],'r--')\n",
    "    plt.xlim([-0.01, 1.01])\n",
    "    plt.ylim([-0.01, 1.01])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig('Log_ROC')\n",
    "    plt.show()\n",
    "    print('\\n')\n",
    "    \n",
    "    # Calculating and plotting Cost Complexity Pruning Path\n",
    "    title('-','Cost Complexity Pruning Path',124)\n",
    "    path=clf.cost_complexity_pruning_path(X_train,y_train)\n",
    "    ccp_alphas,impurities=path.ccp_alphas,path.impurities\n",
    "    fig,ax=plt.subplots()\n",
    "    ax.plot(ccp_alphas[:-1],impurities[:-1],marker='o',drawstyle='steps-post')\n",
    "    plt.xticks(rotation=45, horizontalalignment='right')\n",
    "    ax.set_xlabel('Effective Alpha')\n",
    "    ax.set_ylabel('Total Impurity of Leaves')\n",
    "    ax.set_title('Total Impurity vs Effective Alphas \\n(calculated for the training set)')\n",
    "    plt.show()\n",
    "    print('\\n')\n",
    "\n",
    "    mods=[]\n",
    "    for ccp_alpha in ccp_alphas:\n",
    "        mod=tree.DecisionTreeClassifier(random_state=0,ccp_alpha=ccp_alpha)\n",
    "        mod.fit(X_train,y_train)\n",
    "        mods.append(mod)\n",
    "    print('The number of nodes in the last tree is: \\033[1m{}\\033[0m with ccp_alpha = \\033[1m{}\\033[0m'\n",
    "          .format(mods[-1].tree_.node_count,ccp_alphas[-1]))\n",
    "    mods=mods[:-1]\n",
    "    ccp_alphas=ccp_alphas[:-1]\n",
    "    node_counts=[mod.tree_.node_count for mod in mods]\n",
    "    depth=[mod.tree_.max_depth for mod in mods]\n",
    "    fig,ax=plt.subplots(2,1)\n",
    "    ax[0].plot(ccp_alphas,node_counts,marker='o',drawstyle='steps-post')\n",
    "    ax[0].set_xlabel('Alpha')\n",
    "    ax[0].set_ylabel('Number of Nodes')\n",
    "    ax[0].set_title('Number of Nodes vs Alpha')\n",
    "    ax[1].plot(ccp_alphas,depth,marker='o',drawstyle='steps-post')\n",
    "    ax[1].set_xlabel('Alpha')\n",
    "    ax[1].set_ylabel('Depth of Tree')\n",
    "    ax[1].set_title('Depth vs Alpha')\n",
    "    fig.tight_layout()\n",
    "    plt.xticks(rotation=45, horizontalalignment='right')\n",
    "    plt.show()\n",
    "\n",
    "    # Accuracy vs alpha for training and testing sets\n",
    "    train_scores=[mod.score(X_train,y_train) for mod in mods]\n",
    "    test_scores=[mod.score(X_test,y_test) for mod in mods]\n",
    "    fig,ax=plt.subplots()\n",
    "    ax.set_xlabel('Alpha')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title('Accuracy vs Alpha \\ncalculated for training and testing sets')\n",
    "    ax.plot(ccp_alphas,train_scores,marker='o',label='Training set',drawstyle='steps-post')\n",
    "    ax.plot(ccp_alphas,test_scores,marker='o',label='Testing set',drawstyle='steps-post')\n",
    "    ax.legend()\n",
    "    plt.xticks(rotation=45, horizontalalignment='right')\n",
    "    plt.show()\n",
    "    \n",
    "    d={'Training Scores':train_scores, 'Testing Scores':test_scores,'Alphas':ccp_alphas}\n",
    "    a=pd.DataFrame(d)\n",
    "    print(a)\n",
    "    print('\\n')\n",
    "    \n",
    "    # Generating Decision Tree graph\n",
    "    title('-','Decision Tree graph',124)\n",
    "    with sns.axes_style():\n",
    "        ax=plt.subplots(figsize=(20,15))\n",
    "        tree.plot_tree(decision_tree=clf,\n",
    "                        feature_names=feature_columns,\n",
    "                        class_names=target_column,\n",
    "                        fontsize=10,\n",
    "                        impurity=True,\n",
    "                        filled=True,\n",
    "                        label='all')\n",
    "        plt.title('{m} - Tree Graph'.format(m=model))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DECISION TREE REGRESSOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decisionTree_reg(feature_columns,target_column,data_frame,os_thres,*args,**kwargs):\n",
    "    '''\n",
    "    The decisionTree_reg function is for executing a regressor model based on a certain amount of features of a dataset.\n",
    "    This function has the purpose of returning a complete performance analysis of a certain model. The function will\n",
    "    generate R2 score, mean squared error, root mean squared error, mean absolute error, maximum error and\n",
    "    ROC-AUC curves of the executed model.\n",
    "    The function parameters you should pass are:\n",
    "    - feature_columns: list of str, default=None\n",
    "        It is the list of features that the model will evaluate to predict the target or class\n",
    "    - target_column: str, default=None\n",
    "        It is the target variable or class to predict with the model\n",
    "    - data_frame: str, default=None\n",
    "        It is the name of the dataframe that the feature_columns and the target_column belong to.\n",
    "    - os.thres: float, default=None\n",
    "        Represents the threshold up to which the oversampling won't be needed given the ratio between the minority \n",
    "        and the majority class. The threshold is represented with a float between 0 and 1. \n",
    "    '''\n",
    "\n",
    "    # Importing libraries\n",
    "    import numpy as np                                   # for handling mathematical operations\n",
    "    import pandas as pd                                  # for building and handling dataframe operations\n",
    "    import matplotlib.pyplot as plt                      # for generating plots\n",
    "    import seaborn as sns                                # for generating plots\n",
    "    from sklearn import tree                             # for building the models and generating the tree graph\n",
    "    from imblearn.over_sampling import SMOTE             # for generating an oversampling dataset\n",
    "    from sklearn.model_selection import train_test_split # for slicing the dataframe in testing and training\n",
    "    from sklearn import metrics                          # for returning the performance metrics of the model\n",
    "    from sklearn import preprocessing                    # for \n",
    "        \n",
    "    def title(symbol,string,width):\n",
    "        len_str = len(string)\n",
    "        len_sym = width-len_str\n",
    "        len_sym = int(len_sym/2)\n",
    "        title = symbol*len_sym+string+symbol*len_sym\n",
    "        print(title)\n",
    "    \n",
    "    # Defining target and features variables\n",
    "    X = data_frame[feature_columns]   # Features\n",
    "    y = data_frame[target_column]     # Target\n",
    "    \n",
    "    # Generating Oversampling\n",
    "    min_y=min(list(y.value_counts())) # Minoritary class\n",
    "    max_y=max(list(y.value_counts())) # Majoritary class\n",
    "    min_ratio=min_y/max_y             # ratio of the minoritary class by the majoritary class\n",
    "    title('-','Target variable sampling ratio',124)\n",
    "    if os_thres>min_ratio:\n",
    "        print('Minority Class: '+str(min_y)+\n",
    "              '\\nMajority Class: '+str(max_y)+\n",
    "              '\\nMinority Proportion: '+str(round(min_ratio,2)))\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20) # 70% training and 30% test\n",
    "        os = SMOTE(sampling_strategy='auto',random_state=0,k_neighbors=5)\n",
    "        X_train, y_train = os.fit_resample(X_train, y_train)\n",
    "        title('-','Oversampling generated with SMOTE technique',124)\n",
    "        fig,ax=plt.subplots(1,2,figsize=(10,5))\n",
    "        ax[0].bar(y.value_counts().index,y.value_counts().values,color=['darkorange','blue'],alpha=0.8)\n",
    "        ax[0].set_ylabel('Count')\n",
    "        ax[0].set_xlabel('Class')\n",
    "        ax[0].set_title('Countplot of Each Class before oversampling\\nwithin the Target Variable')\n",
    "        ax[1].bar(y_train.value_counts().index,y_train.value_counts().values,color=['darkorange','blue'],alpha=0.8)\n",
    "        ax[1].set_ylabel('Count')\n",
    "        ax[1].set_xlabel('Class')\n",
    "        ax[1].set_title('Countplot of Each Class after oversampling\\nwithin the Target Variable')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('Minority Class: '+str(min_y)+\n",
    "              '\\nMajority Class: '+str(max_y)+\n",
    "              '\\nMinority Proportion: '+str(round(min_ratio,2)))\n",
    "        title('-','No Oversampling generated',124)\n",
    "        sns.countplot(y)\n",
    "        plt.ylabel('Count')\n",
    "        plt.xlabel('Class')\n",
    "        plt.title('Countplot of Each Class \\nwithin the Target Variable')\n",
    "        plt.show()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30) # 70% training and 30% test\n",
    "    \n",
    "    ## Creating Decision Tree Regressor\n",
    "    model = 'Decision Tree Regressor'\n",
    "    reg = tree.DecisionTreeRegressor(random_state=0,*args,**kwargs)\n",
    "    ## Training Decision Tree Regressor\n",
    "    reg = reg.fit(X_train,y_train)\n",
    "    ## Predicting the response for test dataset\n",
    "    y_pred = reg.predict(X_test)\n",
    "       \n",
    "    # Generating metrics for the executed model\n",
    "    title('-','Model Performance Metrics',124)\n",
    "    print('R2: %0.2f'%round(metrics.r2_score(y_test, y_pred),2))\n",
    "    print('Mean Squared Error: %0.2f'%round(metrics.mean_squared_error(y_test, y_pred),2))\n",
    "    print('Root Mean Squared Error: %0.2f'%round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),2))\n",
    "    print('Mean Absolute Error: %0.2f'%round(metrics.mean_absolute_error(y_test, y_pred),2))\n",
    "    print('Maximum Error: %0.2f'%round(metrics.max_error(y_test, y_pred),2))\n",
    "    print('\\n')\n",
    "    \n",
    "    # Generating and plotting ROC-AUC Curve\n",
    "    title('-','ROC-AUC Curve',124)\n",
    "    logit_roc_auc = metrics.roc_auc_score(y_test, y_pred)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred,pos_label=1)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label='{m} (Area = %0.2f)'.format(m=model)%logit_roc_auc)\n",
    "    plt.plot([0,1],[0,1],'r--')\n",
    "    plt.xlim([-0.01, 1.01])\n",
    "    plt.ylim([-0.01, 1.01])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig('Log_ROC')\n",
    "    plt.show()\n",
    "    print('\\n')\n",
    "    \n",
    "    # Calculating and plotting Cost Complexity Pruning Path\n",
    "    title('-','Cost Complexity Pruning Path',124)\n",
    "    path=reg.cost_complexity_pruning_path(X_train,y_train)\n",
    "    ccp_alphas,impurities=path.ccp_alphas,path.impurities\n",
    "    fig,ax=plt.subplots()\n",
    "    ax.plot(ccp_alphas[:-1],impurities[:-1],marker='o',drawstyle='steps-post')\n",
    "    plt.xticks(rotation=45, horizontalalignment='right')\n",
    "    ax.set_xlabel('Effective Alpha')\n",
    "    ax.set_ylabel('Total Impurity of Leaves')\n",
    "    ax.set_title('Total Impurity vs Effective Alphas \\n(calculated for the training set)')\n",
    "    plt.show()\n",
    "    print('\\n')\n",
    "\n",
    "    mods=[]\n",
    "    for ccp_alpha in ccp_alphas:\n",
    "        mod=tree.DecisionTreeRegressor(random_state=0,ccp_alpha=ccp_alpha)\n",
    "        mod.fit(X_train,y_train)\n",
    "        mods.append(mod)\n",
    "    print('The number of nodes in the last tree is: \\033[1m{}\\033[0m with ccp_alpha = \\033[1m{}\\033[0m'\n",
    "          .format(mods[-1].tree_.node_count,ccp_alphas[-1]))\n",
    "    mods=mods[:-1]\n",
    "    ccp_alphas=ccp_alphas[:-1]\n",
    "    node_counts=[mod.tree_.node_count for mod in mods]\n",
    "    depth=[mod.tree_.max_depth for mod in mods]\n",
    "    fig,ax=plt.subplots(2,1)\n",
    "    ax[0].plot(ccp_alphas,node_counts,marker='o',drawstyle='steps-post')\n",
    "    ax[0].set_xlabel('Alpha')\n",
    "    ax[0].set_ylabel('Number of Nodes')\n",
    "    ax[0].set_title('Number of Nodes vs Alpha')\n",
    "    ax[1].plot(ccp_alphas,depth,marker='o',drawstyle='steps-post')\n",
    "    ax[1].set_xlabel('Alpha')\n",
    "    ax[1].set_ylabel('Depth of Tree')\n",
    "    ax[1].set_title('Depth vs Alpha')\n",
    "    fig.tight_layout()\n",
    "    plt.xticks(rotation=45, horizontalalignment='right')\n",
    "    plt.show()\n",
    "\n",
    "    # Accuracy vs alpha for training and testing sets\n",
    "    train_scores=[mod.score(X_train,y_train) for mod in mods]\n",
    "    test_scores=[mod.score(X_test,y_test) for mod in mods]\n",
    "    fig,ax=plt.subplots()\n",
    "    ax.set_xlabel('Alpha')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title('Accuracy vs Alpha \\ncalculated for training and testing sets')\n",
    "    ax.plot(ccp_alphas,train_scores,marker='o',label='Training set',drawstyle='steps-post')\n",
    "    ax.plot(ccp_alphas,test_scores,marker='o',label='Testing set',drawstyle='steps-post')\n",
    "    ax.legend()\n",
    "    plt.xticks(rotation=45, horizontalalignment='right')\n",
    "    plt.show()\n",
    "    \n",
    "    d={'Training Scores':train_scores, 'Testing Scores':test_scores,'Alphas':ccp_alphas}\n",
    "    a=pd.DataFrame(d)\n",
    "    print(a)\n",
    "    print('\\n')\n",
    "    \n",
    "    # Generating Decision Tree graph\n",
    "    title('-','Decision Tree graph',124)\n",
    "    with sns.axes_style():\n",
    "        ax=plt.subplots(figsize=(20,15))\n",
    "        tree.plot_tree(decision_tree=reg,\n",
    "                        feature_names=feature_columns,\n",
    "                        class_names=target_column,\n",
    "                        fontsize=10,\n",
    "                        impurity=True,\n",
    "                        filled=True,\n",
    "                        label='all')\n",
    "        plt.title('{m} - Tree Graph'.format(m=model))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logReg (feature_columns,target_column,data_frame,os_thres,n,*args,**kwargs):\n",
    "\n",
    "    # Importing libraries\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    from sklearn import metrics\n",
    "    from sklearn.feature_selection import RFE\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import statsmodels.api as sm\n",
    "    \n",
    "    def title(symbol,string,width):\n",
    "        len_str = len(string)\n",
    "        len_sym = width-len_str\n",
    "        len_sym = int(len_sym/2)\n",
    "        title = symbol*len_sym+string+symbol*len_sym\n",
    "        print(title)\n",
    "        \n",
    "    # Defining target and features variables\n",
    "    X = data_frame[feature_columns]   # Features\n",
    "    y = data_frame[target_column]     # Target\n",
    "    \n",
    "    # Checking features to run the model\n",
    "    lr=LogisticRegression(max_iter=10000,random_state=0,*args,**kwargs)\n",
    "    rfe=RFE(lr,n_features_to_select=n)\n",
    "    rfe=rfe.fit(X,y)\n",
    "    support=rfe.support_\n",
    "    ranking=rfe.ranking_\n",
    "    z=zip(feature_columns,support,ranking)\n",
    "    l=list(z)\n",
    "    features=[]\n",
    "    for i in range(len(l)):\n",
    "        if l[i][2]==1:\n",
    "            features.append(l[i][0])\n",
    "    X = data_frame[features]\n",
    "    \n",
    "    # Generating Oversampling\n",
    "    min_y=min(list(y.value_counts())) # Minoritary class\n",
    "    max_y=max(list(y.value_counts())) # Majoritary class\n",
    "    min_ratio=min_y/max_y             # ratio of the minoritary class by the majoritary class\n",
    "    title('-','Target variable sampling ratio',124)\n",
    "    if os_thres>min_ratio:\n",
    "        print('Minority Class: '+str(min_y)+\n",
    "              '\\nMajority Class: '+str(max_y)+\n",
    "              '\\nMinority Proportion: '+str(round(min_ratio,2)))\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20) # 70% training and 30% test\n",
    "        os = SMOTE(sampling_strategy='auto',random_state=0,k_neighbors=5)\n",
    "        X_train, y_train = os.fit_resample(X_train, y_train)\n",
    "        title('-','Oversampling generated with SMOTE technique',124)\n",
    "        fig,ax=plt.subplots(1,2,figsize=(10,5))\n",
    "        ax[0].bar(y.value_counts().index,y.value_counts().values,color=['darkorange','blue'],alpha=0.8)\n",
    "        ax[0].set_ylabel('Count')\n",
    "        ax[0].set_xlabel('Class')\n",
    "        ax[0].set_title('Countplot of Each Class before oversampling\\nwithin the Target Variable')\n",
    "        ax[1].bar(y_train.value_counts().index,y_train.value_counts().values,color=['darkorange','blue'],alpha=0.8)\n",
    "        ax[1].set_ylabel('Count')\n",
    "        ax[1].set_xlabel('Class')\n",
    "        ax[1].set_title('Countplot of Each Class after oversampling\\nwithin the Target Variable')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('Minority Class: '+str(min_y)+\n",
    "              '\\nMajority Class: '+str(max_y)+\n",
    "              '\\nMinority Proportion: '+str(round(min_ratio,2)))\n",
    "        title('-','No Oversampling generated',124)\n",
    "        sns.countplot(y)\n",
    "        plt.ylabel('Count')\n",
    "        plt.xlabel('Class')\n",
    "        plt.title('Countplot of Each Class \\nwithin the Target Variable')\n",
    "        plt.show()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30) # 70% training and 30% test\n",
    "    \n",
    "    # Generating the Logistic Regression model\n",
    "    model = 'Logistic Regression'\n",
    "    sc = StandardScaler()\n",
    "    X_train=sc.fit_transform(X_train)\n",
    "    X_test=sc.fit_transform(X_test)\n",
    "    clf = LogisticRegression(random_state=0,*args,**kwargs)\n",
    "    clf = clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Generating metrics for the executed model\n",
    "    title('-','Model Performance Metrics',124)\n",
    "    print('Accuracy: %0.2f'%round(metrics.accuracy_score(y_test, y_pred),2))\n",
    "    print('Jaccard: %0.2f'%round(metrics.jaccard_score(y_test, y_pred,pos_label=1),2))\n",
    "    print('Precision: %0.2f'%round(metrics.precision_score(y_test, y_pred,pos_label=1),2))\n",
    "    print('Recall: %0.2f'%round(metrics.recall_score(y_test, y_pred,pos_label=1),2))\n",
    "    print('\\n')\n",
    "\n",
    "    # Generating classification report\n",
    "    title('-','Classification Report',124)\n",
    "    print(metrics.classification_report(y_test, y_pred))\n",
    "    print('\\n')\n",
    "     \n",
    "    # Generating Confusion Matrix\n",
    "    title('-','Confusion Matrix',124)\n",
    "    cm_t = metrics.confusion_matrix(y_test, y_pred)\n",
    "    with sns.axes_style(\"white\"):\n",
    "        fig, ax = plt.subplots(figsize=(4,4))\n",
    "        ax = sns.heatmap(cm_t, \n",
    "                        square=True,\n",
    "                        annot=True,\n",
    "                        fmt=\"d\",\n",
    "                        cbar=False)\n",
    "        ax.xaxis.set_ticklabels(['Negative', 'Positive'])\n",
    "        ax.yaxis.set_ticklabels(['Negative', 'Positive']);\n",
    "        ax.set_xlabel('Predicted')\n",
    "        ax.set_ylabel('True')\n",
    "    plt.title('Confusion Matrix of the {m}'.format(m=model))\n",
    "    plt.show()\n",
    "    print('\\n')\n",
    "    \n",
    "    # Generating and plotting ROC-AUC Curve\n",
    "    title('-','ROC-AUC Curve',124)\n",
    "    logit_roc_auc = metrics.roc_auc_score(y_test, y_pred)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred,pos_label=1)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label='{m} (Area = %0.2f)'.format(m=model)%logit_roc_auc)\n",
    "    plt.plot([0,1],[0,1],'r--')\n",
    "    plt.xlim([-0.01, 1.01])\n",
    "    plt.ylim([-0.01, 1.01])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig('Log_ROC')\n",
    "    plt.show()\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANDOM FOREST CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomForest_clf(feature_columns, target_columns, data_frame,maxFeat,numEst,crit):\n",
    "    \n",
    "    # Importing libraries\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    %matplotlib inline\n",
    "    \n",
    "    # Defining target and features variables\n",
    "    X = data_frame[feature_columns]   # Features\n",
    "    y = data_frame[target_column]     # Target\n",
    "\n",
    "    # Generating Oversampling\n",
    "    min_y=min(list(y.value_counts())) # Minoritary class\n",
    "    max_y=max(list(y.value_counts())) # Majoritary class\n",
    "    min_ratio=min_y/max_y             # ratio of the minoritary class by the majoritary class\n",
    "    print('----------Target variable sampling ratio----------')\n",
    "    sns.countplot(y)\n",
    "    plt.ylabel('Count')\n",
    "    plt.xlabel('Class')\n",
    "    plt.title('Countplot of Each Class \\nwithin the Target Variable')\n",
    "    plt.show()\n",
    "    if os_thres>min_ratio:\n",
    "        print('Minority Class: '+str(min_y)+\n",
    "              '\\nMajority Class: '+str(max_y)+\n",
    "              '\\nMinority Proportion: '+str(round(min_ratio,2)))\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30) # 70% training and 30% test\n",
    "        os = SMOTE(sampling_strategy='auto',random_state=0,k_neighbors=5)\n",
    "        X_train, y_train = os.fit_resample(X_train, y_train)\n",
    "        print('----------Oversampling generated with SMOTE technique----------\\n')\n",
    "    else:\n",
    "        print('Minority Class: '+str(min_y)+\n",
    "              '\\nMajority Class: '+str(max_y)+\n",
    "              '\\nMinority Proportion: '+str(round(min_ratio,2)))\n",
    "        print('----------No Oversampling generated----------\\n')\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30) # 70% training and 30% test\n",
    "\n",
    "    # Create a Random Forest Classifier\n",
    "    clf=RandomForestClassifier(random_state=0)\n",
    "    ## Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred=clf.predict(X_test)\n",
    "\n",
    "    feature_imp = pd.Series(clf.feature_importances_*100,index=features).sort_values(ascending=False)\n",
    "    print(feature_imp)\n",
    "    \n",
    "    # Creating a bar plot\n",
    "    sns.barplot(x=feature_imp, y=feature_imp.index)\n",
    "    # Add labels to your graph\n",
    "    plt.xlabel('Feature Importance Score')\n",
    "    plt.ylabel('Features')\n",
    "    plt.title(\"Visualizing Important Features\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # Generating metrics for the executed model\n",
    "    print('----------Model Performance Metrics----------')\n",
    "    print('Accuracy: %0.2f'%round(metrics.accuracy_score(y_test, y_pred),2))\n",
    "    print('Jaccard: %0.2f'%round(metrics.jaccard_score(y_test, y_pred,pos_label=1),2))\n",
    "    print('Precision: %0.2f'%round(metrics.precision_score(y_test, y_pred,pos_label=1),2))\n",
    "    print('Recall: %0.2f'%round(metrics.recall_score(y_test, y_pred,pos_label=1),2))\n",
    "    print('\\n')\n",
    "\n",
    "    # Generating classification report\n",
    "    print('----------Classification Report----------')\n",
    "    print(metrics.classification_report(y_test, y_pred))\n",
    "    print('\\n')\n",
    "     \n",
    "    # Generating Confusion Matrix\n",
    "    print('----------Confusion Matrix----------')\n",
    "    cm_t = metrics.confusion_matrix(y_test, y_pred)\n",
    "    with sns.axes_style(\"white\"):\n",
    "        fig, ax = plt.subplots(figsize=(4,4))\n",
    "        ax = sns.heatmap(cm_t, \n",
    "                        square=True,\n",
    "                        annot=True,\n",
    "                        fmt=\"d\",\n",
    "                        cbar=False)\n",
    "        ax.xaxis.set_ticklabels(['Negative', 'Positive'])\n",
    "        ax.yaxis.set_ticklabels(['Negative', 'Positive']);\n",
    "        ax.set_xlabel('Predicted')\n",
    "        ax.set_ylabel('True')\n",
    "    plt.title('Confusion Matrix of the {m}'.format(m=model))\n",
    "    plt.show()\n",
    "    print('\\n')\n",
    "\n",
    "    # Generating and plotting ROC-AUC Curve\n",
    "    print('----------ROC-AUC Curve----------')\n",
    "    logit_roc_auc = metrics.roc_auc_score(y_test, y_pred)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred,pos_label=1)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label='{m} (Area = %0.2f)'.format(m=model)%logit_roc_auc)\n",
    "    plt.plot([0,1],[0,1],'r--')\n",
    "    plt.xlim([-0.01, 1.01])\n",
    "    plt.ylim([-0.01, 1.01])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUPPORT VECTOR MACHINES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm(feature_columns,target_column,data_frame,C=1.0,gamma=0.01,kernel='rbf'):\n",
    "    \n",
    "    # Importing libraries\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn import svm,svr,svc\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # Defining target and features variables\n",
    "    X = data_frame[feature_columns]   # Features\n",
    "    y = data_frame[target_column]     # Target\n",
    "    \n",
    "    x_min,x_max=x[:,0].min()-1,x[:,0].max()+1\n",
    "    y_min,y_max=x[:,1].min()-1,x[:,1].max()+1\n",
    "    h=(x_max-x_min)/100\n",
    "\n",
    "    xx,yy=np.meshgrid(np.arange(x_min,x_max,h),np.arange(y_min,y_max,h))\n",
    "    \n",
    "    x_plot=np.c_[xx.ravel(),yy.ravel()]\n",
    "    \n",
    "    svc=svm.SVC(kernel=kernel,C=C,gamma=gamma,decision_function_shape='ovr').fit(x,y)\n",
    "    y_pred=svc.predict(x_plot)\n",
    "    y_pred=y_pred.reshape(xx.shape)\n",
    "    plt.figure(figsize=(16,9))\n",
    "    plt.contourf(xx,yy,y_pred,cmap=plt.cm.tab10,alpha=0.3)\n",
    "    plt.scatter(x[:,0],x[:,1],c=y,cmap=plt.cm.tab10)\n",
    "    plt.xlabel('Longitud de los Pétalos')\n",
    "    plt.ylabel('Anchura de los pétalos')\n",
    "    plt.xlim(xx.min(),xx.max())\n",
    "    plt.title('SVC para las flores de Iris con Kernel '+kernel)\n",
    "    plt.show()\n",
    "\n",
    "from ipywidgets import interact, fixed\n",
    "interact(svm_iris,C=[0.01,0.1,1,10,100,1000,1e6,1e10],\n",
    "         gamma=[1e-4,1e-3,1e-2,0.1,0.2,0.5,0.99],kernel=['rbf','linear','poly','sigmoid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONVOLUTIONAL NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "import numpy as np\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Input, Activation, BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Limpiamos el directorio de logs\n",
    "!rm -rf ./logs/ \n",
    "\n",
    "# Definimos hiperparametros\n",
    "batch_size = 100\n",
    "num_classes = 10\n",
    "epochs = 20\n",
    "\n",
    "# Cargamos el dataset y separamos en datos de entrenamiento y en datos de testing\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Asignamos las filas, columnas y canales de la base de imagenes CIFAR10\n",
    "_ ,filas, columnas, canales = X_test.shape\n",
    "print(xt.shape)\n",
    "\n",
    "# Para entrenar es necesario pasar los valores a float32\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Normalizamos para que queden en el rango de 0....1\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255\n",
    "\n",
    "# Las etiquetas las pasamos a valores categoricos one-hot\n",
    "#convertir las clases pues no se puede poner un numero entero a secas -> formato Categorico -> crea array de 10 posiciones, es decir, de 1 a 10 con 1s y 0s\n",
    "#ejemplo: si la etiqueta es un bird -----> [0 0 1 0 0 0 0 0 0 0]  la funcion Categorical convierte el 1 al formato de array (on-hot encoding)\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "opcion = 2\n",
    "\n",
    "if(opcion==1):\n",
    "    Entradas = Input(shape=(filas,columnas,canales))\n",
    "    x = Conv2D(64, kernel_size=(3,3), activation='relu')(Entradas)\n",
    "    x = Conv2D(128, kernel_size=(3,3), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2,2))(x)\n",
    "  \n",
    "    x=Flatten()(x)\n",
    "    x=Dense(10,activation='relu')(x)\n",
    "    x=Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "if(opcion==2):\n",
    "    Entradas=Input(shape=(filas,columnas,canales))\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(Entradas)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    #x = Conv2D(128,kernel_size=(3,3))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(128,kernel_size=(3,3))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    " \n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    #x = Conv2D(256,kernel_size=(3,3))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(256,kernel_size=(3,3))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "  \n",
    "    x = Flatten()(x)\n",
    "    x = Dense(10,activation='relu')(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "modelo = Model(inputs=Entradas, outputs=x)\n",
    "modelo.summary()\n",
    "optimizador = Adam(lr=0.001, beta_1=0.9, beta_2=0.9)  #SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "modelo.compile(loss = categorical_crossentropy, optimizer = optimizador, metrics=['categorical_accuracy'])\n",
    "\n",
    "# Preparamos para visualizar el resultado en el TensorBoard\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = log_dir, histogram_freq=1)\n",
    "\n",
    "modelo.fit(X_train,\n",
    "           y_train, \n",
    "           batch_size = batch_size, \n",
    "           epochs = epochs, \n",
    "           verbose=1, \n",
    "           validation_data = (X_test, y_test), \n",
    "           callbacks=[tensorboard_callback])\n",
    "\n",
    "puntuacion = modelo.evaluate(X_test, y_test, verbose=1)\n",
    "print(puntuacion)\n",
    "\n",
    "%tensorboard --logdir logs/fit\n",
    "# https://www.tensorflow.org/tensorboard/get_started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUTOENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the matplotlib backend so figures can be saved in the background\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "# import the necessary packages\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "\n",
    "\n",
    "class ConvAutoencoder:\n",
    "    @staticmethod\n",
    "    def build(width, height, depth, filters=(32, 64), latentDim=16):\n",
    "\n",
    "      # ------------------------------ENCODER + LATENT SPACE----------------------------------------------\n",
    "\n",
    "      # initialize the input shape to be \"channels last\" along with\n",
    "      # the channels dimension itself\n",
    "      # channels dimension itself\n",
    "      inputShape = (height, width, depth)\n",
    "      chanDim = -1\n",
    "\n",
    "      # define the input to the encoder\n",
    "      inputs = Input(shape=inputShape)\n",
    "      x = inputs\n",
    "\n",
    "      # loop over the number of filters\n",
    "      for f in filters:\n",
    "        # apply a CONV => RELU => BN operation\n",
    "        x = Conv2D(f, (3, 3), strides=2, padding=\"same\")(x)\n",
    "        x = LeakyReLU(alpha=0.2)(x)\n",
    "        x = BatchNormalization(axis=chanDim)(x)\n",
    "\n",
    "      # flatten the network and then construct our latent vector\n",
    "      volumeSize = K.int_shape(x)\n",
    "      x = Flatten()(x)\n",
    "      latent = Dense(latentDim)(x)\n",
    "\n",
    "      # build the encoder model\n",
    "      encoder = Model(inputs, latent, name=\"encoder\")\n",
    "\n",
    "      # ------------------------------LATENT SPACE + DECODER----------------------------------------------\n",
    "\n",
    "      # start building the decoder model which will accept the\n",
    "      # output of the encoder as its inputs\n",
    "      latentInputs = Input(shape=(latentDim,))\n",
    "      x = Dense(np.prod(volumeSize[1:]))(latentInputs)\n",
    "      x = Reshape((volumeSize[1], volumeSize[2], volumeSize[3]))(x)\n",
    "      # loop over our number of filters again, but this time in\n",
    "      # reverse order\n",
    "\n",
    "      for f in filters[::-1]:\n",
    "        # apply a CONV_TRANSPOSE => RELU => BN operation\n",
    "        x = Conv2DTranspose(f, (3, 3), strides=2,\tpadding=\"same\")(x)\n",
    "        x = LeakyReLU(alpha=0.2)(x)\n",
    "        x = BatchNormalization(axis=chanDim)(x)\n",
    "\n",
    "      # apply a single CONV_TRANSPOSE layer used to recover the\n",
    "      # original depth of the image\n",
    "      x = Conv2DTranspose(depth, (3, 3), padding=\"same\")(x)\n",
    "      outputs = Activation(\"sigmoid\")(x)\n",
    "\n",
    "      # build the decoder model\n",
    "      decoder = Model(latentInputs, outputs, name=\"decoder\")\n",
    "\n",
    "      # our autoencoder is the encoder + decoder\n",
    "      autoencoder = Model(inputs, decoder(encoder(inputs)), name=\"autoencoder\")\n",
    "\n",
    "      # return a 3-tuple of the encoder, decoder, and autoencoder\n",
    "      return (encoder, decoder, autoencoder)\n",
    "    \n",
    "    \n",
    "\n",
    "# initialize the number of epochs to train for and batch size\n",
    "EPOCHS = 15\n",
    "BS = 32\n",
    "\n",
    "# load the MNIST dataset\n",
    "print(\"[INFO] loading MNIST dataset...\")\n",
    "((trainX, _), (testX, _)) = mnist.load_data()\n",
    "\n",
    "# add a channel dimension to every image in the dataset, then scale\n",
    "# the pixel intensities to the range [0, 1]\n",
    "trainX = np.expand_dims(trainX, axis=-1)\n",
    "testX = np.expand_dims(testX, axis=-1)\n",
    "\n",
    "trainX = trainX.astype(\"float32\") / 255.0\n",
    "testX = testX.astype(\"float32\") / 255.0\n",
    "\n",
    "# construct our convolutional autoencoder\n",
    "print(\"[INFO] building autoencoder...\")\n",
    "(encoder, decoder, autoencoder) = ConvAutoencoder.build(28, 28, 1)\n",
    "opt = Adam(lr=1e-3)\n",
    "autoencoder.compile(loss=\"mse\", optimizer=opt)\n",
    "\n",
    "# train the convolutional autoencoder\n",
    "H = autoencoder.fit(trainX, trainX, validation_data=(testX, testX), epochs=EPOCHS, batch_size=BS)\n",
    "\n",
    "\n",
    "print(encoder.summary())\n",
    "print(decoder.summary())\n",
    "print(autoencoder.summary())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# construct a plot that plots and saves the training history\n",
    "N = np.arange(0, EPOCHS)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(\"plot.png\")\n",
    "\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "# use the convolutional autoencoder to make predictions on the\n",
    "# testing images, then initialize our list of output images\n",
    "print(\"[INFO] making predictions...\")\n",
    "decoded = autoencoder.predict(testX)\n",
    "outputs = None\n",
    "samples = 10\n",
    "\n",
    "# loop over our number of output samples\n",
    "for i in range(0, samples):\n",
    "\t# grab the original image and reconstructed image\n",
    "\toriginal = (testX[i] * 255).astype(\"uint8\")\n",
    "\trecon = (decoded[i] * 255).astype(\"uint8\")\n",
    " \n",
    "\t# stack the original and reconstructed image side-by-side\n",
    "\toutput = np.hstack([original, recon])\n",
    " \n",
    "\t# if the outputs array is empty, initialize it as the current\n",
    "\t# side-by-side image display\n",
    "\tif outputs is None:\n",
    "\t\toutputs = output\n",
    "\t# otherwise, vertically stack the outputs\n",
    "\telse:\n",
    "\t\toutputs = np.vstack([outputs, output])\n",
    "  \n",
    "# save the outputs image to disk\n",
    "cv2.imwrite(\"output.png\", outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, ZeroPadding2D, LeakyReLU, UpSampling2D, Conv2D\n",
    "#from tensorflow.keras.layers.advanced_activations import LeakyReLU\n",
    "#from tensorflow.keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GAN():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        validity = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, validity)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(256, input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(1024))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "        model.add(Reshape(self.img_shape))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "        \n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Flatten(input_shape=self.img_shape))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = X_train / 127.5 - 1.\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Generate a batch of new images\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Train the generator (to have the discriminator label samples as valid)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_images(epoch)\n",
    "\n",
    "\n",
    "\n",
    "    def sample_images(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        \n",
    "        im = '/content/gdrive/My Drive/ganimages/'\n",
    "        fig.savefig(im + \"%d.png\" % epoch)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    gan = GAN()\n",
    "    gan.train(epochs=10000, batch_size=32, sample_interval=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Input, Flatten, MaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence,text\n",
    "from tensorflow.keras import Model\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "#Variables de entrada\n",
    "batch_size = 64\n",
    "epochs = 3 #30\n",
    "#Embedding. Capa especial que se utiliza en procesado de texto (por lo general). Asigna a las palabras una vectorización densa y única\n",
    "tamano_embedding = 32\n",
    "#Máximo de palabras caracteristicas que va a tener nuestra BD\n",
    "maximas_palabras = 5000\n",
    "#Longitud máxima de los comentarios que vamos a tener en nuestra BD\n",
    "maxima_longitud = 500\n",
    "\n",
    "#Cargado de bases  de datos\n",
    "(xentrenamiento, yentrenamiento),(xtest, ytest) = imdb.load_data(num_words=maximas_palabras)\n",
    "\n",
    "#recortamos las secuencias a la longitud maxima_longitud. Tanto en entrenamiento como en test\n",
    "xentrenamiento = sequence.pad_sequences(xentrenamiento, maxlen=maxima_longitud)\n",
    "xtest = sequence.pad_sequences(xtest, maxlen=maxima_longitud)\n",
    "\n",
    "#ya está vectorizado! \n",
    "# la BD imdb lo trae vectorizado! sino, deberiamos vectorizar las palabras usando la libreria Sklearn u otra\n",
    "print(xentrenamiento[:10])\n",
    "print(yentrenamiento[:10])\n",
    "\n",
    "\n",
    "\n",
    "#Creacion de modelo\n",
    "entrada = Input(shape=(maxima_longitud, ))\n",
    "x = Embedding(maximas_palabras, tamano_embedding)(entrada)\n",
    "#x = Dropout(0.2)(x)\n",
    "#x = LSTM(100, return_sequences=True, activation='relu')(x)\n",
    "x = LSTM(100, dropout=0.2, recurrent_dropout=0.2)(x)\n",
    "#x = Dropout(0.2)(x)\n",
    "#x = Flatten()(x)\n",
    "x = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "modelo = Model(inputs=entrada, outputs=x)\n",
    "modelo.compile(loss='binary_crossentropy',\n",
    "               optimizer='adam',\n",
    "               metrics=['binary_accuracy'])\n",
    "modelo.summary()\n",
    "\n",
    "#Entrenamiento\n",
    "#Usamos la callback para guardar el mejor modelo. Mejor modelo que armó en las épocas\n",
    "checkpoint = ModelCheckpoint('comentarios.h5',\n",
    "                             monitor='val_binary_accuracy',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=False,\n",
    "                             mode='auto')\n",
    "history=modelo.fit(xentrenamiento, \n",
    "                   yentrenamiento, \n",
    "                   batch_size=batch_size, \n",
    "                   epochs=epochs, \n",
    "                   callbacks=[checkpoint], \n",
    "                   validation_data=(xtest,ytest), \n",
    "                   verbose=1)\n",
    "\n",
    "# Evaluamos el modelo\n",
    "scores = modelo.evaluate(xtest, ytest, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "\n",
    "#visualizacion de resultaoos\n",
    "plt.figure(1)\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Perdidas del Modelo')\n",
    "plt.ylabel('Perdidas')\n",
    "plt.xlabel('Epocas')\n",
    "plt.legend(['Test','Entrenamiento'], loc='upper left')\n",
    "\n",
    "plt.figure(2)\n",
    "plt.plot(history.history['val_binary_accuracy'])\n",
    "plt.plot(history.history['binary_accuracy'])\n",
    "plt.title('Precision del Modelo')\n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel('Epocas')\n",
    "plt.legend(['Test','Entrenamiento'], loc='upper left')\n",
    "\n",
    "\n",
    "\n",
    "modelo.load_weights('comentarios.h5')\n",
    "print(xentrenamiento[:5])\n",
    "print(yentrenamiento[:5])\n",
    "predicciones = modelo.predict(xentrenamiento[:5],verbose=1)\n",
    "print(predicciones)\n",
    "\n",
    "#COMENTARIO NEGATIVO (definimos como < 0.5)\n",
    "res_pred = predicciones[predicciones<0.5]\n",
    "print(res_pred)\n",
    "\n",
    "\n",
    "#COMENTARIO POSITIVO (definimos como >= 0.5)\n",
    "res_pred = predicciones[predicciones>=0.5]\n",
    "print(res_pred)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
